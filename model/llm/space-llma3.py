import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# T·∫£i model
model_id = "meta-llama/Llama-3.1-8B-Instruct"  # ho·∫∑c model nh·ªè h∆°n n·∫øu b·ªã OOM
tokenizer = AutoTokenizer.from_pretrained(model_id)
pipe = pipeline(
    "text-generation",
    model=model_id,
    tokenizer=tokenizer,
    device_map="auto",        # t·ª± ƒë·ªông ch·ªçn GPU/CPU
    torch_dtype="auto"        # t·ª± ch·ªçn dtype (FP16 n·∫øu GPU h·ªó tr·ª£)
)

# H√†m tr·∫£ l·ªùi
def chat(user_input, history):
    messages = [{"role": "system", "content": "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªØu √≠ch."}]
    for h in history:
        messages.append({"role": "user", "content": h[0]})
        messages.append({"role": "assistant", "content": h[1]})
    messages.append({"role": "user", "content": user_input})

    response = pipe(messages, max_new_tokens=300, do_sample=True, temperature=0.7)
    answer = response[0]["generated_text"][-1]["content"] if isinstance(response[0]["generated_text"], list) else response[0]["generated_text"]

    history.append((user_input, answer))
    return history, history

# UI Gradio
with gr.Blocks() as demo:
    gr.Markdown("# üöÄ Chatbot ch·∫°y b·∫±ng LLaMA tr√™n Hugging Face Space")
    chatbot = gr.Chatbot()
    msg = gr.Textbox(placeholder="Nh·∫≠p tin nh·∫Øn...")
    clear = gr.Button("Xo√° h·ªôi tho·∫°i")

    state = gr.State([])

    msg.submit(chat, [msg, state], [chatbot, state])
    clear.click(lambda: ([], []), None, [chatbot, state], queue=False)

demo.launch()
